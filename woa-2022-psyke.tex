\documentclass[
% twocolumn,
% hf,
]{ceurart}

\usepackage{woa-2022-psyke}

\sloppy

\begin{document}
\copyrightyear{2022}
\copyrightclause{Copyright for this paper by its authors.
	Use permitted under Creative Commons License Attribution 4.0
	International (CC BY 4.0).}

\conference{WOA 2022: 23rd Workshop From Objects to Agents, September 1--2, Genova, Italy}

%%
\title{Hypercube-based methods for symbolic knowledge extraction: Towards a unified model}

%%
\author[1]{Federico Sabbatini}[%
orcid=0000-0002-0532-6777,
email=f.sabbatini1@studio.uniurb.it,
url=http://federicosabbatini.apice.unibo.it
]\cormark[1]
\address[1]{Dipartimento di Scienze Pure e Applicate (DiSPeA), Universit\`a di Urbino, Italy}

\author[2]{Giovanni Ciatto}[%
orcid=0000-0002-1841-8996,
email=giovanni.ciatto@unibo.it,
url=https://about.me/gciatto
]
\address[2]{Dipartimento di Informatica -- Scienza e Ingegneria (DISI), \textsc{Alma Mater Studiorum}---Universit\`a di Bologna, Italy}

\author[3]{Roberta Calegari}[%
orcid=0000-0003-3794-2942,
email=roberta.calegari@unibo.it,
url=http://robertacalegari.apice.unibo.it
]
\address[3]{Alma Mater Research Institute for Human-Centered Artificial Intelligence, \textsc{Alma Mater Studiorum}---Universit\`a di Bologna, Italy}

\author[2]{Andrea Omicini}[%
orcid=0000-0002-6655-3869,
email=andrea.omicini@unibo.it,
url=http://andreaomicini.apice.unibo.it
]

\cortext[1]{Corresponding author.}

\begin{abstract}
Symbolic knowledge-extraction (SKE) algorithms proposed by the XAI community to obtain human-intelligible explanations for opaque machine learning predictors are currently being studied and developed with growing interest, also in order to achieve believability in interactions.
%
However, choosing the most adequate extraction procedure amongst the many existing in the literature is becoming more and more challenging, as the amount of available methods increases.
%
In fact, most of the proposed algorithms come with \emph{constraints} over their applicability.

In this paper we focus upon a quite general class of SKE techniques, namely \emph{hypercube-based} methods.
%
Despite being commonly considered as regression-specific, we discuss why hypercube-based SKE methods are flexible enough to deal with classification problems as well.
%
More generally, we propose a common generalised model for hypercube-based methods, and we show how they can be exploited to perform SKE on datasets, predictors, or learning tasks of any sort.
%
% We show that further manipulations may be horizontally enabled for a large class of knowledge extractors without the need to revise their algorithms, but only adding our proposed generalisation.
%
% We also report as a concrete example the implementation of the proposed generalisation in the \psyke{} framework.
\end{abstract}

%%
%% Keywords.
\begin{keywords}
	explainable AI 
	\sep
  	knowledge extraction 
	\sep
	interpretable prediction 
	\sep
  	PSyKE
\end{keywords}

\maketitle

\section{Introduction}

One of the main features to be underpinned by a believable human-agent interaction is the capability of being \emph{explainable}. 
%
Along this line, \emph{symbolic knowledge extraction} (SKE) is a powerful tool within the scope of explainable artificial intelligence (XAI).
%
It enables reverse-engineering of the black-box (BB) machine learning algorithms---which are nowadays exploited in many AI tasks~\cite{rocha2012far}.
%
SKE allows data scientists to associate human-comprehensible, \emph{post-hoc} explanations~\cite{KENNY2021103459} to the recommendations or decisions computed by the most common prediction-effective -- yet, poorly interpretable -- algorithms.
%
To cite some examples, SKE is widely adopted to credit-risk evaluation \cite{baesens2003using,baesens2001building,steiner2006using}, medical diagnosis -- i.e., to make early breast cancer prognosis predictions \cite{franco2007early} and for recognising hepatobiliary disorders \cite{hayashi2000comparison} or other diseases and dysfunctions \cite{bologna1997three} --, credit card screening \cite{setiono2011rule}, intrusion detection systems \cite{hofmann2003rule}, keyword extraction \cite{azcarraga2012keyword}, and space mission data prediction~\cite{sabbatini22LPFSKE}.

The basic idea behind SKE is to construct \emph{symbolic} (hence, interpretable) models mimicking the behaviour of the pre-existing black-box predictors to be explained.
%
Such symbolic models should describe the corresponding black boxes in terms of the outputs they provide as responses to (classes of) inputs values.
%
Symbols, in particular, may consist of intelligible knowledge, e.g., lists or trees of \emph{logic rules} that can be exploited to obtain predictions as well as to better understand the underlying predictor.
%
In other words, symbols are both human- and machine-interpretable.

Because of the many SKE techniques available in the literature, selecting the most appropriate SKE algorithm for any given learning task may easily become cumbersome.
%
Difficulties may arise because of the intrinsic design choices behind each extraction algorithm.
%
In fact, SKE algorithms may commonly target specific learning tasks (classification or regression), specific sorts of ML predictors (e.g.\ neural networks, SVM, linear models, etc.), or specific sorts of training data (e.g.\ continuous, categorical, or binary).

In this paper we focus upon a quite general class of SKE techniques, namely \emph{hypercube-based} methods.
%
Methods of such sort extract symbolic knowledge by querying black-box predictors as oracles, and by recursively partitioning the input spaces of these black boxes into several hypercubes---hence following a divide-et-impera approach.
%
Despite being commonly considered regression-specific, we show that hypercube-based SKE methods are flexible enough to deal with classification problems as well.
%
More generally, we propose a common model for hypercube-based methods, and we show how they can be exploited to perform SKE on data sets, predictors, or learning tasks of any sort.

Accordingly, the contribution of this paper is manifold.
%
First, we provide a general and abstract description of any hypercube-based SKE workflow.
%
Second, we compare existing hypercube-based methods (e.g.\ \iter{}~\cite{huysmans2006iter} and \gridex{}~\cite{gridex-extraamas2021}) based on the way they partition the input space, approximate black-box decisions, and construct the extracted symbolic knowledge.
%
Third, we discuss how hypercube-based methods can be engineered to provide full support to supervised learning tasks---there including regression and classification ones.
%
% Finally, to demonstrate the versatility of hypercube-based methods, we analyse disparate SKE scenarios via the \psyke{} framework~\cite{psyke-woa2021,psyke-ia2022,psyke-extraamas2022}---that is, a platform combining SKE and Semantic Web to provide human-interpretability and intelligent agent-interoperability for BB-based machine learning tasks.

The remainder of this paper is organised as follows.
%
\xs{state} describes the state of the art for SKE as well as some background notions to fully understand the work.
%
\xs{contribution} presents our model for hypercube-based methods and compares the most relevant methods from the literature. %, which are then exemplified in \xs{experiments}.
%
Finally, conclusions are drawn in \xs{conclusions}.

\section{State of the Art}\labelsec{state}

In the remainder of this section we provide readers with some details about the state of the art for symbolic knowledge extraction. % (\xss{extraction}) and the \psyke{} platform (\xss{psyke}).

\subsection{Knowledge Extraction}\labelssec{extraction}

Computational systems are considered \emph{interpretable} when humans are able to easily understand its operation and outcomes~\cite{agentbasedxai-aamas2020}.
%
However, nowadays decision support systems often rely on ML models having excellent predictive capabilities at the expense of interpretability.
%
These \emph{sub-symbolic} predictors of growing complexity, which learn input-output relations from data and store them as internal parameters, do not provide any kind of \emph{symbolic} representation of the acquired knowledge, thus lacking of an interpretable representation to the benefit of human users.
%
ML algorithms are defined as \emph{black boxes} for this reason~\cite{Lipton2018}.

It is possible to preserve the impressive BB predictive performance and, at the same time, obtain human-intelligible clues or explanations regarding the BB behaviour by substituting the opaque model with a mimicking interpretable surrogate.
%
The \xai{} community, indeed, have proposed a number of means to produce \emph{ex-post} explanations for sub-symbolic predictors in the form of surrogate models based on sets of rules extracted from the underlying opaque model.
%
Amongst the proposals there are methods to extract lists~\cite{craven1994using,huysmans2006iter,gridex-extraamas2021} or trees~\cite{craven1996extracting,breiman1984classification} of logic rules, usually if-then-else, \mofn{} or fuzzy.
%
SKE is particularly important also for another reason: it may enable further manipulations, for instance to merge the \emph{know-how} of different BB models \cite{xmas-aiiot2019}.

Knowledge extraction algorithms can be categorised along three orthogonal dimensions~\cite{xaisurvey-ia14}:
%
\begin{inlinelist}
    \item\label{item:category:learning-task} supported learning tasks,
    \item\label{item:category:knowledge-form} shape of the symbolic knowledge provided in output,
    \item\label{item:category:translucency} the supported underlying ML models, usually known as \emph{translucency}.
\end{inlinelist}

Supported tasks -- \cref{item:category:learning-task} -- are usually supervised classification or regression.
%
To the best of our knowledge, so far only supervised machine learning tasks have been considered by SKE, thus neglecting unsupervised or reinforcement learning tasks.
%
A cluster of SKE algorithms can only explain BB classifiers -- e.g.\ Rule-extraction-as-learning~\cite{craven1994using}, \trepan~\cite{craven1996extracting} and others \cite{barakat2005eclectic,martens2007comprehensible} --, while a different cluster is designed to support BB regressors---e.g., \iter~\cite{huysmans2006iter}, \gridex~\cite{gridex-extraamas2021}, \gridrex{}~\cite{gridrex-kr2022} and others~\cite{setiono2002extraction,schmitz1999ann,saito2002extracting}.
%
Finally, a little subset of SKE techniques are able to handle both tasks, as for the case of \textsc{G-Rex}~\cite{grex-icdm2008} and \cart~\cite{breiman1984classification}.

As for the shape of the output knowledge -- \cref{item:category:knowledge-form} --, decision rules \cite{freitas2014comprehensible,huysmans2011empirical,murphy1991id2} and trees \cite{quinlan1993c4,simplifyingdt-ijmms27} are usually considered the most human-understandable ways to represent knowledge.
%
For this reason the majority of SKE methods produce one of these two structures as output.
%
Regardless of the shape, conditions describing decision rules and nodes are expressed by using the same input/output data types adopted to train the underlying BB.
%
For instance, SKE procedures applied to classifiers accepting $N$-dimensional numerical data and providing $K$ distinct output classes will produce rule lists or trees involving a certain number of \emph{predicates} over $N$ input variables $x_1, \ldots, x_n$ and having $K$ possible outcomes.
%
A further categorisation may be performed w.r.t.\ the kind of predicates contained in the output knowledge.
%
In particular, it is possible to observe conjunctions or disjunctions of inequalities (e.g.\ $x_i \gtrless c$) as well as inclusions in or exclusions from intervals (e.g.\ $x_i \in [l, u]$) for numerical data.
%
Categorical data are usually associated to equalities (e.g.\ $x_i = c$) and set-inclusions (e.g.\ $x_i \in \{c_1, c_2, \ldots \}$).
%
\mofn{} or fuzzy rules are other available alternatives for boolean/discrete or continuous data, respectively.

Finally, the translucency dimension (\cref{item:category:translucency}) represents the strategy adopted by the SKE algorithm to obtain interpretable knowledge from a BB.
%
In particular, extractors may be \emph{decompositional} or \emph{pedagogical} \cite{andrews1995survey,xaisurvey-ia14}. % or \emph{eclectic}.
%
Decompositional techniques consider the internal structure of the underlying black box, hence producing symbolic knowledge which mimics how it internally works.
%
As a side-effect, decompositional algorithms are bound to specific sorts of ML predictors---and possibly introduce constraints on their internal structures.
%
For instance, techniques tailored on neural networks are not applicable to support-vector machines.
%
Similarly, procedures explicitly designed for 3-layered networks are not suitable for deeper ones.
%
On the other hand, pedagogical methods can extract symbolic knowledge without relying on any information about the inner structure of predictors. 
%
They simply query the predictor as an oracle, observing its response to particular inputs, and generalise its behaviour accordingly.
%
For this reason, pedagogical extractors come with no constraints on sorts of predictors they can be applied to.
%
Hence, they are more general -- despite potentially less precise --, but considerations about the output performance strictly depend on the task at hand.
%
% Finally, eclectic procedures mix elements of the two aforementioned categories.
% %
% Since some kind of internal structure inspection is performed, it is possible to encompass them in the wider decompositional category.

To evaluate the quality of SKE techniques different indicators are exploited, depending on the task to solve.
%
Common choices are readability, fidelity and predictive performance measurements \cite{towell1993extracting}.
%
The former expresses how interpretable is the output knowledge from the human perspective.
%
It is generally evaluated through the number of extracted rules and the number of constraints per rule.
%
Fidelity is related to the capability of the extracted knowledge to mimic the underlying BB predictions, whereas predictive performance measurements are assessed by comparing the predictions drawn from the extracted knowledge with the expected data.
%
Measurements involving predictions should be assessed via the same scoring function used for the underlying BB---which in turn strictly depends on the performed task.
%
Classifiers are usually evaluated via accuracy, precision, recall, and F$_1$ score.
%
Conversely, common metrics for regressors are the mean absolute/squared error (MAE/MSE) and the R${^2}$ score.

% \subsection{\psyke}\labelssec{psyke}

% \begin{figure}
% \centering
% 	\includegraphics[width=\linewidth]{figures/Psyke.pdf}
% 	\caption{\psyke{} design.}
% 	\labelfig{psyke-design}
% \end{figure}

% \input{tables/tab-algos.tex}

% The \psyke{}~\cite{psyke-woa2021,psyke-ia2022} software library is a Python framework providing general-purpose support to SKE.
% %
% It can be exploited to obtain logic rules from BB classifiers and regressors via several pedagogical extraction methods.
% %
% Its unified API allow users to select the most adequate procedure with few lines of code, also allowing fast comparison between different alternatives.
% %
% At the time of writing, \psyke{} supports 6 state-of-the-art SKE algorithms, reported on the top of \xf{psyke-design} (see \xt{psyke} for further details), allowing researchers and data scientists to exploit them without the need to implement and test them.

% As shown in \xf{psyke-design}, the \psyke{} design is developed around the notion of \emph{extractor}, intended as any algorithm accepting as input a ML predictor (classifier or regressor) together with the data set used to train it, and providing as output a \emph{theory} of \emph{logic} rules extracted from the predictor.
% %
% \psyke{} extractors need additional information about the data set to give more human-interpretability to the extracted knowledge.
% In particular, a schema of the data set can be given as input to formally describe input and output feature names and types.

% \psyke{} also exhibits utilities to manipulate the data set and perform feature engineering, for instance procedures to discretise or scale continuous features and to one-hot encode discrete/discretised features.
% %
% In addition, there are automatic procedures to select the optimal parameters for extractors, which manual tuning may be challenging for human users.

% As for the knowledge provided in output by extractors, it is possible to choose between two options:
% \begin{inlinelist}
% 	\item a Prolog theory composed of human-intelligible clauses, possibly simplified to ease readability;
% 	\item an OWL ontology having agent-interpretable SWRL rules, to pursue interoperability between intelligent agents~\cite{psyke-extraamas2022}.
% \end{inlinelist}
% %
% Input data as well may follow Semantic Web encoding, so \psyke{} extractors accept tabular data or knowledge graphs stored in OWL ontologies.

\section{Hypercube-based knowledge extractors}\labelsec{contribution}

In this section we present a general hypercube-based extractor model (\xss{unified-model}).
%
We then delve into the details of different algorithms from the literature, discussing how they match the model (\xss{existing-methods}).

\subsection{Unified Model}\labelssec{unified-model}

Hypercube-base extraction methods are \emph{pedagogical} extraction procedures which can operate on trained ML predictors of any sort.
%
They consider the predictor $P$ undergoing extraction as an oracle to be queried multiple times, in order to find a partitioning $H_1 \cup \ldots \cup H_n$ of its input space $\mathcal{X}$ such that the output space $\mathcal{Y}$ can be concisely %and correctly 
expressed for each partition.
%
Hence, they extract knowledge in the form of rule lists or trees, where each rule attempts to describe the outcome of $P$ for a particular hypercube $H_i \subseteq \mathcal{X}$. 
%
Rules have the following logical form: 
%
\[ \mathbf{x} \in H_i \rightarrow (~\mathbf{y} = f_i(\mathbf{x})~) \]  
%
to be read as ``if the input vector $\mathbf{x} \in \mathcal{X}$ is in some hypercube $H_i$, then the prediction $\mathbf{y} \in \mathcal{Y}$ is $f_i(\mathbf{x})$'', where $f_i(\mathbf{x})$ is a function that approximates $P$ outcomes.
%
Note that each hypercube $H_i$ is a partition of the input space $\mathcal{X}$, and $f_i$ is the function approximating $P$ outcomes related to that hypercube (could be a costant, a linear function, etc.), i.e.: $f_i(\mathbf{x}) \approx P(\mathbf{x}),~\forall \mathbf{x} \in H_i$.

Hypercube-based extraction procedures should then attempt to select hypercubes and local approximation functions so as to maximise the fidelity of the overall rule set/list w.r.t.\ $P$.
%
Accordingly, they follow a pretty linear workflow, which may be roughly summarised in 3 steps, namely:
%
\begin{enumerate}
	\item partitioning the input space into disjoint hypercubes $H_1 \cup \ldots \cup H_n$, following a selected strategy and according to possible defined constraints;
	%
	\begin{itemize}
		\item e.g. one may be willing to minimise $n$ while maximising the size of each $H_i$
	\end{itemize}
	\item approximating the prediction of $P$ for each hypercube $H_i$, via some function $f_i$; and
	%
	\begin{itemize}
		\item e.g. one may be willing to maximise the similarity among $P$ and $f_i$ in $H_i$
	\end{itemize}
	\item creating a rule set where each rule \emph{concisely} represents the behaviour of $P$ in $H_i$ via $f_i$.
\end{enumerate}
%
In the following, we delve into the details of all the aforementioned phases.

\subsubsection{Input space partitioning}

Input space partitioning is a recursive computation aimed at finding the optimal number, shape and size of hypercubes w.r.t.\ some \emph{desiderata}, such as:
%
\begin{inlinelist}
	\item covering the whole input space;
	\item obtaining disjoint regions;
	\item minimising the number of regions;
	\item maximising the similarity amongst the samples inside single regions;
	\item minimising the predictive error correlated to each partition.
\end{inlinelist}

These conditions cannot be all satisfied simultaneously, especially when dealing with high-dimensional data sets.
%
Thus, some requirements may be relaxed.
%
For instance, the input space coverage may be limited only to \emph{interesting} hypercubes, neglecting the others.
%
Let us consider a hypercube `interesting' if it contains training samples, as it may have a role to play in drawing future predictions.

Alternatively, the partitioning process may terminate after a predefined number of iterations, as some state-of-the-art algorithms actually do.
%
However, this may lead to the indiscriminate exclusion of some regions of the input space that, conversely, are not negligible.
%
In its turn, the explained model will not to be able to provide predictions for a subset of input instances.

Non-contiguous hypercubes may be relaxed into hierarchical or fuzzy regions, possibly mapped into non-overlapping rules, in order to have unambiguous output predictions.

\paragraph{Similarity and fidelity}

The amount of hypercubes an input space is partitioned into may significantly impact the interpretability of the final symbolic model.
%
In fact, hypercube-based methods will output as many rules as the hypercubes they have partitioned the input space into---and of course more (or more complex) rules imply lower readability for the human reader.

The capability of grouping together similar samples into a single hypercube is so quintessential for supporting the creation of few, general, and simple rules which capture the behaviour of the original predictor with high fidelity.
%
This corresponds to
%
\begin{inlinelist}
	\item data points from the same hypercube drawing \emph{similar} predictions, and to
	\item predictions having a high \emph{fidelity} (or, equivalently, low error rates) w.r.t. to the original predictor.
\end{inlinelist}
%
Accordingly, here we delve into the details of how to assess
%
\begin{inlinelist}
	\item similarity amongst data points from \emph{contiguous} hypercubes, as well as
	\item predictive errors between a candidate rule and the underlying predictor.
\end{inlinelist}

\subparagraph{Similarity amongst instances}

Input space partitions may be considered similar according to the following definitions:
%
\begin{description}
	\item[input closeness] if the input variables of both subregions have values ranging in similar domains, as for the case of adjacent disjoint or overlapping regions;
	\item[output closeness] if the output associated with the instances in the two subregions may be defined as similar.
\end{description}
%
While it is straightforward to check input closeness (e.g., through Euclidean distance), dealing with output closeness requires taking into account the learning problem at hand.

As far as classification is concerned, we may consider two hypercubes $H_1$ and $H_2$ as \emph{output-close} w.r.t. a predictor $P$ if (and only if) the most frequent output class is the same in both hypercubes:
%
\begin{equation}\label{eq:simClass}
	H_1 \stackrel{P}{\approx} H_2 \Leftrightarrow \text{mode}(P(H_1)) = \text{mode}(P(H_2))\,
\end{equation}
%
where $\text{mode}(\cdot)$ denotes the statistical operator returning the most frequent item over a set, and $P(H)$ is a shortcut standing for $\{ P(\textbf{x}) : \textbf{x} \in H \}$, to lighten the notation.

Conversely, in the case of regression with constant outputs, output-similarity may be expressed as a function of the absolute difference between the mean output predictions performed by the predictor $P$ on the two hypercubes $H_1$ and $H_2$:
%
\begin{equation}\label{eq:simRegK}
	H_1 \stackrel{P}{\approx} H_2 \Leftrightarrow |\text{mean}(P(H_1)) - \text{mean}(P(H_2))| < \theta\,
\end{equation}
%
where $\theta$ is a parameter defining the strictness of the similarity criterion.

Of course, definition \ref{eq:simRegK} is not suitable to capture the similarity amongst hypercubes characterised by high variability of $P$.
%
In such a case a more complex solution is required:
%
\begin{equation}\label{eq:simRegLin}
	H_1 \stackrel{P}{\approx} H_2 \Leftrightarrow \text{mae}(f_{1,2}, H_1 \cup H_2) \leq (\text{mae}(f_1, H_1) + \text{mae}(f_2, H_2)) / 2 \,
\end{equation}
%
where $\text{mae}(f, H)$ is the mean absolute error of the linear function $f$ in approximating 
%the input/output relationship 
$P$ for data in $H$.
%
In other words, $H_1$ and $H_2$ are output-similar if it is possible to
%
\begin{inlinelist}
	\item merge the two hypercubes, %and to
	\item find a linear combination $f_{1,2}$ of the input variables representing the input/output relationship of the so merged regions, and
	\item reach a predictive performance of $f_{1,2}$ better than the average performance of the linear functions $f_1, f_2$ associated with the corresponding separated subregions.
\end{inlinelist}

\input{figures/fig-gen-sim.tex}

For instance, in \xf{general-sim} we report examples of similarity assessments calculated for a generalised extractor applied to a classification task (\xf{gs1}) and to a regression task (\xff{gs2}{gs3}).
%
Figures concerning the regression task represent constant and non-constant extractor outputs, respectively.
%
The example assumes a 2-dimensional data set with continuous input features both ranging in the interval [0, 5].
%
In the figures, hypercubes to be expanded/merged are those having coloured backgrounds.
%
Possible adjacent hypercubes to be joined to them are represented as hypercubes having no background.
%
Adjacent hypercubes that are similar to the hypercubes to be expanded are represented with hatched background.
%
It is worth noting that for the example depicted in \xf{gs2} a similarity threshold $\theta$ equal to 5.0 has been chosen.
%
In \xf{gs3} the predictive errors corresponding to the adjacent hypercubes as well as the calculated errors of the possible merged regions are omitted for the clarity of the image.

\subparagraph{Predictive error assessment}

A generalised metric is necessary to evaluate the predictive performance of a set of rules for both classifications and regressions.
%
We propose the following function as error function for a rule set $R$ applied to a data set $D$:
%
\begin{equation}\label{eq:error}
	\text{error}(R, D) = 
	\begin{cases}
		\text{mae}(R, D) & \text{(regression)}\\
		1 - \text{accuracy}(R, D) & \text{(classification)}\\
	\end{cases}\,
\end{equation}
%
where $\text{mae}(R, D)$ and $\text{accuracy}(R, D)$ are the mean absolute error and the classification accuracy score, respectively, calculated on the output predictions obtained via the rules in $R$, for the data set $D$, and w.r.t.\ the expected outputs for $D$.

\input{figures/fig-gen-err.tex}

In \xf{general-error} we report some examples of predictive errors measured for a generalised extractor by assuming a 2-dimensional data set with continuous input features both ranging in the [0, 5] interval.
%
The figure represents a classification task and two regression tasks.
%
The first regression task is approximated by the extractor with constant outputs, whereas the second is associated with non-constant outputs.
%
The predictive error is reported as misclassifications in the first case and absolute error in the others.
 
\subsubsection{Approximating predictions}

As for the approximation of output predictions associated with each hypercube, they are usually computed on the basis of the predictions provided by the underlying black box when applied to an \emph{extended} training set.
%
The extended training set may consist of the original data the predictor has been trained upon -- or a subset of it --, possibly augmented with some further data.
%
Data augmentation via random input samples is useful to attain higher predictive performance, provided that the predictor is used as an oracle to compute the corresponding expected outputs.

Provided that the input space has been adequately partitioned into several hypercubes, the prediction associated with each hypercube may consist of 
%
\begin{inlinelist}
	\item a constant numerical value (e.g., \iter{}, \gridex{}) or, 
	\item a linear combination of the input variables (e.g., \gridrex{}).
\end{inlinelist}
%
The latter option, in particular, is well-suited for regression tasks, while the former may support both classification and regression tasks.

To choose the best output value corresponding to each hypercube, one may either 
%
\begin{inlinelist}
	\item aggregate the predictions corresponding to all available points in that hypercube -- e.g. via the `mean', `mode', or `median' statistical aggregation functions --, or
	\item fit a local function \emph{locally} approximating the predictor in that hypercube.
\end{inlinelist}
%
Again, which option is better really depends on the learning task the underlying predictor has been designed for.
%
% The aggregation level of the computation depends on the hypercube associated to the rule.
% %
% Indeed, usually these computations are performed by considering only training instances \emph{inside} a cube, so the resulting rule is \emph{local}.
% %
% A factorisation of the common operations for both scenarios (constant and non-constant outputs) highlights that the creation of output values for extractors dedicated to regression tasks only differs in the raw aggregated computation described above.
% %
% As a consequence, the two possible computations may be exchanged without introducing any other alteration in a generic extractor, making it more versatile.

% It is possible to enlarge the applicability scope of these extractors to classification tasks thanks to the following consideration.
% %
% A classification rule based on a hypercube may be associated to a constant output value, that is the string label to be provided as prediction for every instance inside the cube.
% %
% The label may trivially be the most common label predicted by the underlying predictor applied to all the instances inside the cube.
% %
% This generalisation enable the adoption for classification tasks of SKE techniques explicitly designed for regression.

\input{figures/fig-gen-pred.tex}

In \xf{general-pred}, we report examples of predictions provided by a generalised extractor.
%
As for the previous examples, a 2-dimensional data set with continuous input features both ranging in the [0, 5] interval is assumed.
%
The Figure shows a classification task and two regression tasks, where the former regression task is approximated by the extractor with constant outputs.
%
The background colour represents the output provided by the extractor.

\subsubsection{Output rules set creation}\labelssec{rule}

After selecting a set of input space regions and one output decision for each of them, hypercube-based extractors build a set of rules where each one is composed of a precondition and a postcondition.
%
The precondition is a formal description of a single input region in terms of individual features, for instance by means of value inclusion inside an interval.
%
Hypercubic $n$-dimensional regions may be described through the conjunction of (at most) $n$ interval inclusion conditions.
%
On the other hand, the postcondition is simply the decision calculated for the region on the basis of the task at hand, as previously described.
%
Thus, extracted human-readable logic rules generally have the following format:
%
\begin{equation*}
	\text{Output is ~} O \text{~ if ~} X_1 \in [l_1, u_1], X_2 \in [l_2, u_2], ..., X_n \in [l_n, u_n],
\end{equation*}
%
where $O$ is the output decision and $X_1, X_2, ..., X_n$ are input variables assuming values included in the intervals described by corresponding lower-bounds $l_i$ and upper-bounds $u_i$.

\subsection{Comparison of Existing Methods}\labelssec{existing-methods}

In the following a comparison between the two hypercube-based SKE algorithms %cited in \xs{experiments} 
-- namely \iter{} and \gridex{} -- is provided to give a practical demonstration of our methodology.
%
Differences in the input space partitioning and in the decision approximation are highlighted in particular.
%
Output rules are lists of logic rules in both cases, following the convention described in \xf{rule}.
\gctodo{Sta figura che cos'è?}
%
It worths noting that both algorithms assume input features to be continuous and may be applied to any kind of BB predictor, being pedagogical SKE methods.

\paragraph{\iter}

The \iter{} algorithm~\cite{huysmans2006iter} is based on the iterative creation and expansion of hypercubes inside the input feature space, until a maximum number of iterations is reached or, otherwise, the whole input space is covered.
%
The expansion may terminate also if it is not possible to further expand the hypercubes.
%
In those cases additional cubes may be created to cover the remaining space.

\iter{} is limited to regression tasks by design, and performs averaging operations to associate output values to hypercubes.
%
For each cube, \iter{} selects all the training samples inside it and calculates the mean prediction by using the underlying BB as an oracle.
%
If the training samples are not enough to satisfy the minimum amount specified by the user, extra random samples are generated and predicted together with the others.

\iter{} also takes advantage of a similarity criterion to expand the hypercubes.
%
In particular, at every iteration all the possible expansions around each cube are considered, but only one is performed, i.e., the one capable of expanding a cube towards the most similar input space region.
%
Similarity is calculated via mean absolute difference between the output values of the cubes to be expanded and the eligible cubes around them.

\paragraph{\gridex{}}

The \gridex{} algorithm~\cite{gridex-extraamas2021} may be considered as an extension of \iter{} aimed at overcoming its major drawback, i.e., the non-exhaustivity of its output rules.
%
\gridex{} achieves this goal because it is exhaustive by design.
%
Unlike \iter{}, \gridex{} adopts a top-down approach to split the input feature space into hypercubes.
%
It iteratively partitions the \emph{whole} space according to some defined strategy, marking at each iteration if the created partitions are \emph{negligible} (i.e., they contain no training samples, so they are discarded since it is not relevant to have rules associated to them), \emph{eligible} for further partitioning (if they contain samples that are not enough \emph{similar}), or \emph{permanent} (otherwise, if they contain similar training instances and, thus, these cubes should have a good predictive performance).
%
Strategies to split the input space are \emph{fixed}, if the user specifies for each iterations how many partitions have to be performed along all the input dimensions, or \emph{adaptive}, if the number of splits is determined through the relevance of each input feature w.r.t.\ the output variable.
%
Since \gridex{} has been designed exclusively for regression tasks, as \iter{}, also in this case output decisions are calculated via local averaging calculations and actual regression rules are not supported.

Similarity between samples is assessed through the output value standard deviation of all the instances included inside a hypercube.
%
If the standard deviation is below a user-defined threshold, then the cube only contains similar samples and it is no further partitioned.
%
Otherwise, \gridex{} attempts to split the cube in smaller regions, possibly enclosing more similar samples.
%
Since the readability of the output model depends on the number of extracted rules, it is of paramount importance to keep it as low as possible.
%
For this reason a merging phase is performed after every splitting iteration as an optimisation to reduce the number of rules.
%
Indeed, adjacent cubes are pairwise merged according to a similarity criterion on the contained samples.
%
The merging phase is iterative: at each step are merged only the two adjacent cubes resulting in the merged hypercube having the lowest standard deviation, and it terminates when it is not possible to further merge cubes without exceeding the standard deviation user-defined threshold.

A first \gridex{} generalisation supporting regression rules as output decisions has led to the \gridrex{} algorithm~\cite{gridrex-kr2022}.
%
\gridrex{} can extract fully regressive rules, with a linear combination of the input variables as a postcondition.
%
Even though all the other details are identical to \gridex{}, \gridrex{} is able to achieve better predictive performance, fidelity, and readability than \gridex{}.

% \section{Case Study: \psyke}\labelsec{experiments}

% We exemplify the effectiveness of hypercube-based methods by considering the implementations of the \iter{} and \gridex{} extraction algorithms.
% %
% Both methods are designed for regression and here applied to classification tasks.
% %
% \cart{} is used as benchmark to assess the predictive performance of the modified extractors, since it is a state-of-the-art procedure directly applicable to data sets described by continuous features, without prior discretisation.
% %
% All the adopted implementations are included in the \psyke{} framework.\footnote{Code of experiments is available at \url{https://github.com/psykei/psyke-python}}

% Experiments are executed on the well-known Iris data set,\footnotetext{\vurl{https://archive.ics.uci.edu/ml/datasets/iris}} composed of 150 instances corresponding to Iris flower individuals.
% %
% Each instance is described by 4 continuous input features (i.e., petal and sepal width and length of the exemplary) and a categorical class label (i.e., the species of the exemplary).
% %
% Three different species are present in the Iris data set (namely, Setosa, Virginica, and Versicolor) and they are equally balanced (50 individuals per species).

% The experiments are carried out as follows:
% %
% \begin{inlinelist}
% 	\item the data set is randomly split into training and test sets, of equal size;
% 	\item a $k$-nearest neighbour ($k$-NN) classifier is trained on the training set;
% 	\item three different extractors are used to extract symbolic rules out of that classifier;
% 	\item the predictive performance of both the classifier and the extracted rules are graphically compared -- in terms of decision boundaries --, and numerically assessed---in terms of accuracy and F$_1$ scores. In the case of the extracted rules, fidelity and readability measurements are performed as well.
% \end{inlinelist}
% %
% It worths noticing that the training set is used only to train the models.
% %
% Conversely, the test set is used only to assess the predictive performance of predictor and extractors.
% %
% Both sets are constant for each experiment, to better compare the performance under the same conditions.
% %
% The fidelity of the extracted rules (w.r.t.\ the predictor they have been extracted from) is assessed as well, via the same metrics adopted for the predictive performance.
% %
% Finally, the output knowledge readability is expressed as number of extracted rules.

% \subsection{Predictor training}

% Extraction techniques require an underlying BB to be used as an oracle.
% %
% For this reason we trained and compared several $k$-NN classifiers, with different values for the $k$ hyper-parameter.
% %
% Details about the accuracy and F$_1$ scores measured for each model are reported in \xt{predictor}.
% %
% The best predictive performance is achieved by the 9-NN.
% %
% Consequently, in the following all the discussed extractors are applied to it.
% %
% The decision boundaries of the selected 9-NN are reported in \xf{knn}.

% \input{tables/tab-predictor.tex}

% \subsection{\cart{}}

% \input{tables/tab-cart}

% The \cart{} extractor is applied to the 9-NN classifier to extract human-intelligible knowledge in Prolog syntax, without discretising the input data set.
% %
% Unlike \iter{} and \gridex{}, \cart{} is able to work upon discretised data sets too.
% %
% Training the model with a maximum leaf amount of 3 gives the following theory, composed of 3 rules---namely, one per each possible class of the Iris data set.

% \prologimport{listings/iris-rules-cart.pl}

% The theory is always exhaustive, since it is always possible to find a leaf classifying an instance.
% %
% Numerical assessments about the predictive performance and fidelity of the theory extracted with \cart{} are reported in \xt{cart}.
% %
% The input space partitioning induced by the theory is reported in \xf{cart}.
% %
% In this example, only petal width and length are considered to assign class labels to input instances.

% \subsection{\iter{}}

% \input{tables/tab-iter}

% The \iter{} algorithm has been applied as well to explain the 9-NN classifier.
% %
% We test several hyper-parameter values in order to attain the rule list having the highest possible predictive performance and fidelity.
% %
% \iter{} is based on the following hyper-parameters:
% %
% \begin{inlinelist}
% 	\item the size for updating cubes, expressed as fraction of input dimension (i.e., 0.1 means a tenth of the interval between minimum and maximum values of each dimension);
% 	\item the number of starting points, representing the initial hypercubes;
% 	\item the minimum number of examples to consider in each cube;
% 	\item the similarity threshold between adjacent cubes, that is not relevant for classification;
% 	\item the maximum number of iterations, fixed to 600.
% \end{inlinelist}

% The results of our experiments for \iter{} are reported in \xt{iter}.
% %
% The best predictive performance, achieved with the parameters highlighted in bold font in the Table, corresponds to the following rules.
% %
% \prologimport{listings/iris-rules-iter.pl}
% %
% The input space partitioning induced by the extracted rules is shown in \xf{iter}.

% \subsection{\gridex{}}

% \input{tables/tab-gridex}

% Finally, as last step of our experiments, the \gridex{} extractor is applied to the 9-NN classifier.
% %
% In this case as well different values for the hyper-parameters are been explored.
% %
% We recall that fundamental hyper-parameters for \gridex{} are
% %
% \begin{inlinelist}
% 	\item the depth of the recursive partitioning (i.e., how many iterations);
% 	\item the number of slices to perform at every iteration;
% 	\item the error threshold used to decide if further divide a hypercube, fixed to 0.1 for all experiments;
% 	\item the minimum number of examples to consider in each cube, here fixed to 1.
% \end{inlinelist}
% %
% As for the number of slices to be performed, adaptive strategies are preferred to fixed strategies.
% %
% Experiment results concerning \gridex{} are reported in \xt{gridex}.
% %
% The best hyper-parameter values are highlighted in bold font.
% %
% The semantics of adaptive splitting strategies described by the couple $(a, b)$ is the following: all input dimensions having relevance greater than $a$ are split into $b$ subregions at each iteration.
% %
% All the other dimensions are not split.
% %
% Input feature relevance is always scaled in the [0, 1] interval.
% %
% Corresponding output Prolog theory and input space partitioning are reported in the following and in \xf{gridex}, respectively.

% \prologimport{listings/iris-rules-gridex.pl}

% Also in this case only one input feature is considered to draw predictions.
% %
% The partitioning is exhaustive w.r.t.\ the data set, however a small input space region is neglected since the algorithm observed no instances included in it.
% %
% Differently from \iter{}, \gridex{} is able to detect input dimensions that do not affect the classification.
% %
% In this manner all the non-relevant antecedents are dropped from the output theory, resulting in a higher human-readability.

% \subsection{Discussion}

% \input{figures/fig-extractors.tex}

% In this Subsection we compare the results of \cart{}, \iter{}, and \gridex{} applied to the Iris data set, all summarised in \xf{extractors}.
% %
% Results are compared on the basis of readability, fidelity and predictive performance, other than the decision boundaries induced by the extracted rules.
% %
% As for readability, all the extractors are equivalent w.r.t.\ the number of rules, since they are able to extract one predictive rule per output class.
% %
% Conversely, the readability of \iter{} is hindered by the number of antecedents per rule, since it produces a constraint for each input dimension.
% %
% Under this perspective, \cart{} and \gridex{} are able to keep amongst the rules' conditions only those involving relevant features to perform the classification, resulting in a fourth of the total amount of antecedents w.r.t.\ \iter{}.

% The decision boundaries provided by \gridex{} and \iter{} are more similar to those produced by the underlying $k$-NN, but no sensible difference in the classification accuracy is noticeable, since all extractors present a score between 0.94 and 0.95 (we recall that the 9-NN has an accuracy score equal to 0.97).
% %
% A similar reasoning may be performed about the extractors' fidelity, equal to 0.95 for \cart{} and to 0.97 for \iter{} and \gridex{}.

% It is worth noting that \gridex{} does not provide a classification rule for a small input space region, since it finds that region as negligible (since no data set instances belong to it).

\section{Conclusions}\labelsec{conclusions}

In this paper we generalise a class of SKE techniques, namely hypercube-based methods, to make them suitable for classification tasks as well as regression tasks.
%
We do so by proposing a common model for these methods, %and by concretely implementing our generalisation in the \psyke{} framework.
%
% The proposed generalised model considers in which ways it is possible to 
%relax 
where algorithmic patterns currently adopted by hypercube-based SKE algorithms are relaxed, in order to widen their applicability scopes.
%
Our future works will focus on extending the proposed generalisation to a wider class of SKE techniques, to enable a higher degree of usability for SKE techniques existing in the literature.

\begin{acknowledgments}
	This paper has been partially supported by
	%
	\begin{inlinelist}
		\item the European Union's Horizon 2020 research and innovation programme under G.A.\ no.\ 101017142 (StairwAI project), and by
		\item the CHIST-ERA IV project CHIST-ERA-19-XAI-005, co-funded by the EU and the Italian MUR (Ministry for University and Research).
	\end{inlinelist}
\end{acknowledgments}

\bibliography{woa-2022-psyke}

\end{document}

%%
%% End of file
